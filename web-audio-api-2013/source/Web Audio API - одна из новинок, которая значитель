Web Audio API - одна из новинок, которая значительно расширяет возможности web приложений при работе со звуком. Это мощнейший инструмент без которого Вам сложно будет обойтись в будущем при разработке современных игр и интерактивных веб приложений. API достаточно высокоуровневый, продуман до мелочей, самодостаточен, легок в освоении и особенно элегантно интегрируется в приложения использующие WebGl и WebRTC.

## Немного истори...

<img src="/blogdemo/webaudioapi/img/evolution.png" width="500" alt="web audio api vs tag audio" class="aligncenter" />

Давным давно на заре развития веб, Internet explorer предпринял робкую попытку разрушить тишину царящую в браузере придумал тэг `<bgsound>`, который позволял автоматически проигрывать midi файлы при открытии сайта. В ответ на это разработчики Netscape добавили аналогичную функцию с помощью тега `<embed>` . Ни одно из этих решений так и не было стандартизовано, как в принципе и не было впоследствии наследовано остальными браузерами.</p>

Прошло несколько лет и в браузерах начали активно использоваться сторонние плагины. Проигрывать аудио стало возможным с помощью Flash, Silverlight, QuickTime и т.п. Все они хорошо выполняли свою роль, но все же плагин имеет кучу недостатков и возможность иметь инструмент для работы со звуком, поддерживаемый веб стандартами, уже давно будоражила умы разработчиков. С массовым приходом мобильных браузеров, не поддерживающих Flash, проблема стала еще острее.

Пионером в борьбе с тишиной без плагинов стал элемент `<audio>` появившийся уже в первой спецификации html5. Он позволяет проигрывать аудио файлы и стримы, контролировать воспроизведение, буферизацию и уровень звука. Прост в использовании и понимании. Сейчас он поддерживается всеми мобильными и десктопными браузерами (включая IE9), работает почти отлично, **но говорить сегодня мы будем не об `<audio>` элементе.**

Мы поговорим о Web Audio API, который призван выполнять гораздо более интересные, разносторонние и сложные задачи.

### **web audio API** - это **НЕ** элемент `<audio>` и **НЕ** его надстройка.

В начале важно понять, что `элемент <audio>` и `web Audio API` практически никак не связаны между собой. Это 2 взаимонезависимых самодостаточных API, предназначенные для разных задач. Единственная связь между ними состоит в том, что `<audio>` элемент может быть одним из источников звука для `web Audio API`.

<img src="/blogdemo/webaudioapi/img/api-ws-tag.png" width="400" height="400" alt="web audio api vs tag audio" class="aligncenter" />

**Задачи которые призван решать элемент `<audio>`:**

*   Простой аудио плеер
*   Однопоточное фоновое аудио.
*   Аудио подсказки, капчи и т.п.

**Задачи которые призван решать `Web Audio API`:**

*   Объемный звук для игр и интерактивных веб приложений
*   Приложения для обработки звука
*   Аудио синтез
*   Визуализация аудио и многое, многое, многое другое...

### Приимущества Web Audio API

*   **Абсолютно синхронное воспроизведение аудио** (возможность проигрывать сотни семплов одновременно с разницей в миллисекунды, точно планируя начало и конец воспроизведения каждого из них)
*   **Возможность обработки звука** с помощью десятков встроенных высокоуровневых блоков (фильтров, усилителей, линий задержки, модулей свертки, и т.д.)
*   Богатые возможности для **синтеза колебаний звуковой частоты** с различной формой огибающей. (Можно написать простейший синтезатор за 10 мин)
*   **Работа с многоканальным аудио** (Исходя из спецификации API обязан поддерживать до 32 каналов аудио!!! Для справки: стерео - это 2 канала, Dolby Digital - это 5 каналов, самый навороченный Dolby TrueHD - 8 каналов, т.е на сегодняшний день не у многих пользователей дома есть звуковые карты с более чем 8-ю каналами :)
*   **Непосредственный доступ к временным и спектральным характеристикам сигнала** (Позволяет делать визуализации и анализ аудио потока)
*   **Высокоуровневое 3D распределение аудио по каналам** в зависимости от положения, направления и скорости источника звука и слушателя (Что особенно круто при разработке объемных WebGL игр и приложений)
*   **Тесная интеграция с WebRTC** (Как источник звука можно использовать системный микрофон, подключить гитару или микшер. Вы так-же можете получить аудио из любого внешнего стрима, как впрочем и отправить его туда-же)

### Проблемы на текущий момент (09.2013)

1.  **API все еще находится в стадии черновика и немного меняется**. В большинстве своем это изменения в названиях методов и наборе параметров. Например еще пол года назад для того что-бы начать воспроизведение нужно было использовать `source.noteOn(0)`, сейчас это `source.start(0)`. Проблема небольшая и вы можете использовать обертку <a rel="nofollow" target="blank" href="https://github.com/cwilso/AudioContext-MonkeyPatch">AudioContext-MonkeyPatch</a>, которую Вы конечно можете поддержать своими пулл реквестами.
2.  **Поддержка браузерами**. На сегодняшний день Chrome, Safari, Opera, FF25+, Chrome android, Safari iOS, поддерживают Web Audio API в полном объеме. IE и некоторые мобильные браузеры думают о поддержке в "ближайшем" будущем. Подробнне можно посмотреть тут <a rel="nofollow" target="blank" href="http://caniuse.com/#feat=audio-api">caniuse</a>.
3.  Пока **не существует хорошего универсального аудио формата** который можно не задумываясь использовать в web приложениях. Форматов много (mp3, mp4, wma, ogg, aac, WebM,...) и браузеров тоже много. При этом каждый браузер пытается продвигать свой набор форматов. Подробнее можно посмотреть <a rel="nofollow" target="blank" href="https://developer.mozilla.org/en-US/docs/HTML/Supported_media_formats">тут</a>. В итоге ни один из перечисленных форматов не поддерживается всеми браузерами (09.2013). Для решения проблемы Вам иногда могут понадобится одни и те-же аудио семплы представленные в разных форматах для разных браузеров.

## Начинаем погружение. **Audio context**

Одним из основологающих понятий при работе с Web Audio API является аудио контекст.

    var context = new AudioContext();


Пока в спецификация находится в черновике, в webkit браузерах нужно использовать webkitAudioContext. Т.е что-то наподобие:

    var context;
    window.addEventListener('load', function(){
      try {
        window.AudioContext = window.AudioContext||window.webkitAudioContext;
        context = new AudioContext();
      }
      catch(e) {
        alert('Opps.. Your browser do not support audio API');
      }
    }, false);


У одного документа может быть только один контекст. Этого вполне достаточно для всего спектра задач решаемого Web Audio API. Наличие одного аудио контекста позволяет строить сколь угодно сложные аудио графы с неограниченым количеством источников и получателей звукового сигнала. Практически все методы и конструкторы для создания аудио модулей - являются методами аудио контекста.

### Возможные источники звукового сигнала:

1.  `AudioBufferSourceNode` - аудио буффер (рассмотрим ниже)
2.  `MediaElementAudioSourceNode` - `<audio>` или `<video>` элемент
3.  `MediaStreamAudioSourceNode` - внешний аудио поток (стрим) (микрофон или любой другой аудио стрим, в том числе внешний)

### Возможные получатели звукового сигнала:

1.  `context.destination` - системный звуковой выход по умолчанию (в типичном случае - колонки).
2.  `MediaStreamAudioDestinationNode` - аудио поток (стрим). Этот поток может быть использован таким же образом, как полученный через MediaStream через `getUserMedia()` и, к примеру, может быть отправлен на удаленный RTCPeerConnection с помощью метода `addStream()`.

## Строим графы (схемы обработки аудио)

В любой задуманной вами схеме может быть один или несколько источников и получателей звукового сигнала, а так-же модули для работы со звуком (далее мы рассмотрим каждый из них подробнее). Схема может быть с прямыми и обратными связями, каждый модуль может иметь сколь угодно много входов/выходов. Всю заботу о правильном функционировании берет на себя API. Ваша задача только соединить все правильно. Давайте представим себе некую абстрактную схему, просто что-бы разобраться как она строится при помощи кода.

<img src="/blogdemo/webaudioapi/svg/connect.svg" width="440" alt="connect nodes web audio api" class="aligncenter" />

Создатели Web Audio API сделали построение любых графов изящным и простым для понимания. У каждого модуля есть метод `.connect(...)` который принимает один параметр, собственно говорящий к чему нужно подсоединиться. Вот все что нужно написать для построения вышеупомянутой схемы:

#### JS

    source1.connect(node1);
    source2.connect(node3);
    node1.connect(node4);
    node1.connect(node2);
    node2.connect(destination);
    node3.connect(node1);
    node4.connect(destination);
    node4.connect(node3);


## Предзагрузка аудио и воспроизведение

Давайте рассмотрим простейший, но довольно типовой пример работы с web Audio API, где источником звукового сигнала является буффер созданный из аудио файла предзагруженного с помощью XMLHttpRequest (AJAX), а получателем является системный звуковой выход.

    // создаем аудио контекст
    var context = new window.AudioContext(); //
    // переменные для буфера, источника и получателя
    var buffer, source, destination;

    // функция для подгрузки файла в буфер
    var loadSoundFile = function(url) {
      // делаем XMLHttpRequest (AJAX) на сервер
      var xhr = new XMLHttpRequest();
      xhr.open('GET', url, true);
      xhr.responseType = 'arraybuffer'; // важно
      xhr.onload = function(e) {
        // декодируем бинарный ответ
        context.decodeAudioData(this.response,
        function(decodedArrayBuffer) {
          // получаем декодированный буфер
          buffer = decodedArrayBuffer;
        }, function(e) {
          console.log('Error decoding file', e);
        });
      };
      xhr.send();
    }

    // функция начала воспроизведения
    var play = function(){
      // создаем источник
      source = context.createBufferSource();
      // подключаем буфер к источнику
      source.buffer = buffer;
      // дефолтный получатель звука
      destination = context.destination;
      // подключаем источник к получателю
      source.connect(destination);
      // воспроизводим
      source.start(0);
    }

    // функция остановки воспроизведения
    var stop = function(){
      source.stop(0);
    }

    loadSoundFile('example.mp3');


Давайте попробуем этот код в действии. (Этот и все остальные примеры работают в webkit, что-бы объяснить основные принципы API. Целью статьи не было сделать их рабочими везде :)

<a href="/blogdemo/webaudioapi/article-from-buffer2.html" target="blank" class="demo" data-height="150">Web audio simple</a>

## Модули

Web Audio API содержит десятки готовых к использованию, высокоуровневых, конфигурируемых модулей. Это усилители, линии задержки, фильтры, модули свертки, сплитеры и мержеры каналов, 3D распределители и т.д. Вы можете создавать сложнейшие тракты обработки и синтеза звука просто соединяя готовые блоки и конфигурируя их. По простоте использования это немного напоминает детский конструктор, но в отличии от него, здесь вы можете создавать очень крутые штуки! <img src="/blogdemo/webaudioapi/img/lego-to-mixer-web-audio-api.png" width="400" height="140" alt="построение схем web audio api" class="aligncenter" /> Давайте рассмотрим основные модули начиная с самых простых.

### Gain (Усилитель)

**Модуль позволяет изменять уровень звукового сигнала.**

Любой модуль Web Audio API, можно создать, используя соответствующий конструктор объекта context. Так, для того, что-бы получить новый объект `gain` нужно просто вызвать `context.createGain()`. Далее вы можете конфигурировать полученный объект как до начала, так и во время воспроизведения. Конфигурация, a так-же ее возможности и способы зависят от типа модуля, но в большинстве случаев все сводится к простой установке значений для соответствующих полей объекта. Вот пример того, как создать модуль `gain` и изменить его уровень усиления.

#### JS

    var gainNode = context.createGain();
    gainNode.gain.value = 0.4; // значение 0..1 (можно изменять динамически)


Вставляем усилитель в вышеописанную схему, между источником и получателем:

#### JS

    source.connect(gainNode);
    gainNode.connect(destination);


И начинаем воспроизведение:

#### JS

    source.start(0);


<a href="/blogdemo/webaudioapi/article-gain.html" target="blank" class="demo" data-height="210">Web audio gain node</a>

Как вы уже поняли - все действительно просто и продумано. Для более наглядного примера, давайте сделаем простой crossfade эффект, которым можно управлять вручную с помощю слайрера. Нам понадобится 2 источника звука и 2 модуля `gain`.

<a href="/blogdemo/webaudioapi/article-crossfade.html" target="blank" class="demo" data-height="345">Web audio crossfade effect</a>

Мы не будем приводить код примера дабы не засорять статью, однако вы всегда можете открыть пример в новом окне, проинспектировать и посмотреть как он работает.

### Delay (Линия задержки)

**Этот модуль позволяет зедерживать звук на определенное вами время.**

Создается и конфигурируется по такому-же принципу как вышеописанный gain.

#### JS

    var delayNode = context.createDelay();
    delayNode.delayTime.value = 2; // 2 секунды

    source.connect(delayNode);
    delayNode.connect(destination);
    source.start(0);


<a href="/blogdemo/webaudioapi/article-delay.html" target="blank" class="demo" data-height="210">Web audio delay node</a>

Давайте для закрепления основных принципов создадим просую схему с бесконечным зацикливанием сигнала используя `gain` для ослабления сигнала и `delay` для задержки. Так мы получим простейший "эхо" эффект.

    source.connect(gainNode);
    gainNode.connect(destination);
    gainNode.connect(delayNode);
    delayNode.connect(gainNode);

    var now = context.currentTime;
    source.start(now);
    source.stop(now + 0.3);


<a href="/blogdemo/webaudioapi/article-delay-gain-echo.html" target="blank" class="demo" data-height="245">Web audio delay and gain echo</a>

Надо сказать, что это не самый лучший образец того, как нужно делать "эхо" эффект и годится он только в качестве примера. Настоящее реалистичное эхо можно достигнуть с помощью модуля свертки звукового сигнала. Давайте рассмотрим его подробнее.

### Convolution ( Свертка )

Говоря простым языком, свертка – это математическая операция, такая же как сложение, умножение или интегрирование. При сложении из двух исходных чисел получается третье, при свертке из двух исходных сигналов получается третий сигнал. В теории линейных систем свертка используется для описания **отношений между тремя сигналами**:

*   входным сигналом
*   импульсной характеристикой
*   выходным сигналом

Другими словами, выходной сигнал равен свертке входного сигнала с импульсной характеристикой системы.

<img src="http://html5.by/blogdemo/webaudioapi/img/article-convolution.png" width="600" alt="convolution web audio api. Результат свертки с импульсной характеристикой простого одиночного эха" class="aligncenter" />

Что такое входной и выходной сигнал вроде и так понятно. Осталось только разобраться со "страшным" словом `импульсная характеристика (impulse response)` :)

Давайте рассмотрим жизненный пример и все сразу станет ясно.

Вы пришли в лес. Громко крикнули что-нибудь своему другу. Что он услышит? Правильно! Ваш голос, только немного искаженный и с эффектом множественного эха. Дело в том что савокупность аккустических колебаний генерируемое вашими связками и гортанью, будет несколько изменена под воздействием окружающего пространства, прежде чем попасть в ушную раковину Вашего друга. Преломления и искажения возникнут из-за влажности в лесу. Определенная часть энергии аккустического колебания будет поглащена мягким покрытием из мха. Также звук будет отражен от сотен деревьев и окружающих вас предметов находящихся на разном удалении. Можно еще долго перечислять все эти факторы, но давайте разберемся в том, какое отношение все это имеет к свертке :)

Вы уже наверное поняли, что в описанной ситуации, входным сигналом (источником сигнала) - будет то, что кричите Вы. Выходным же сигналом будет то, что слышит Ваш друг. А вот лес можно представить себе как линейную систему, способную изменять характеристики сигнала по неким правилам, зависящим от огромного набора факторов. Не вникая в теорию, всю эту савокупность правил можно представить в виде так называемой импульсной характеристики.

Эхо в пещере, специфический шум при проигрывании старой пластинки, искажения голоса водителя троллейбуса ворчащего в старый микрофон: все эти звуковые эффекты можно однозначно представить их импульсными характеристиками.

Вот небольшая демка. Все очень просто. Переключая эффекты, вы всего лишь изменяете ту самую импульсную характеристику, которая является основным параметром для модуля свертки.

<a href="/blogdemo/webaudioapi/article-convolution.html" target="blank" class="demo" data-height="210">Web audio convolution</a>

Модуль свертки создается, подключается и конфигурируется точно так-же, как и все остальные.

#### JS

    convolverNode = context.createConvolver();
    convolverNode.buffer = buffer; // impulse response

    source.connect(convolverNode);
    convolverNode.connect();


Практически во всех типовых случаях, нужная Вам импульсная характеристика - это обычный аудио файл (чаще всего .wav). Как и входной сигнал, она должна быть предзагружена, декодирована и записана в буфер.

Где найти импульсные характеристики для различных эффектов? Ищите в гугле что-то типа "download free impulse response" и найдете их в огромном количестве. Например <a href="http://www.voxengo.com/impulses/" target="blank" rel="nofollow">тут</a>, <a href="http://www.audioease.com/IR/" target="blank" rel="nofollow">тут</a> или <a href="http://www.freesound.org/search/?q=impulse+response&f=&s=score+desc&advanced=0&g=1" target="blank" rel="nofollow">тут</a>.

В конце статьи будет несколько ссылок, для желающих разобраться со сверткой подробнее. Движемся дальше и переходим к не менее интересной теме - частотной фильтрации в Web Audio API.

### Filter ( Фильтрация )

Под фильтрацией в цифровой обработке сигналов чаще всего подразумевают частотную фильтрацию. Если вы знаете, что такое спектр сигнала, преобразование Фурье и АЧХ фильтра, то просто посмотрите пример. Если же вы вообще не в курсе, что это такое и времени разбираться нет, попробую объяснить на пальцах.

<img src="/blogdemo/webaudioapi/img/equalizer.png" width="600" alt="web audio api equalizer and frequancy response" class="aligncenter" />

Все пользовались эквалайзером в любимом winamp, aimp, itunes и т.п., наверняка пробовали разные предустановленные режимы (бас, диско, вокал) и обязательно дергали ползунки на разных частотах, пытаясь добиться нужного звучания. Эквалайзер представляет собой устройство, которое может как усилить, так и ослабить определенные частотные составляющие звука. Любой звук (за исключением некоторых, рожденных искуственным путем) состоит из основной частоты + множества гармоник, то есть любой сигнал состоит из суммы множества частот. Ну а при помощи эквалайзера мы можем управлять громкостью этих самых частотных составляющих звука.

Так вот, не вдаваясь в детали: - *Эквалайзер* - это частотный фильтр - *Кривая образованная всеми ползунками* - это АЧХ (амплитудно-частотная характеристика) фильтра, а по английски frequency response function.

Т.е. говоря простым языком, с помощью Web Audio API вы можете добавить такой "эквалайзер" (фильтр) в свой граф обработки сигнала в виде модуля.

С настройкой амплитудно-частотной характеристики все будет немного сложнее. Дело в том, что исторически все распространенные типы фильтров уже имеют физические аналоги у которых есть определенные параметры, характеризующие эти фильтры.

Вот список фильтров доступных из коробки:

1.  **lowpass** - фильтр нижних частот (обрезает все, что выше выбранной вами частоты)
2.  **highpass** - высокочастотный фильтр (обрезает все, что ниже выбранной вами частоты)
3.  **bandpass** - полосовой фильтр (пропускает только определенную полосу частот)
4.  **lowshelf** - полка на низких частотах (означает, что усиливаются или ослабляется все что ниже выбранной вами частоты, включая ее)
5.  **highshelf** - полка на высоких частотах (означает, что усиливаются или ослабляется все что выше выбранной вами частоты, включая ее)
6.  **peaking** - узкополосный пиковый фильтр (усиливает определенную частоту, народное название - "фильтр-колокол")
7.  **notch** - узкополосный режекторный фильтр (ослабляет определенную частоту, народное название - "фильтр-пробка")
8.  **allpass** - фильтр, пропускающий все частоты сигнала с равным усилением, однако изменяющий фазу сигнала. Происходит это при изменении задержки пропускания по частотам. Обычно такой фильтр описывается одним параметром — частотой, на которой фазовый сдвиг достигает 90°.

Для того что-бы настраивать эти фильтры существует несколько параметров, которые, как мы уже сказали есть у физических аналогов фильтров. Эти параметры можно найти во всех книжках по теории обработки сигналов.

*   **Frequency** - (частота) - частота на которой базируется фильтр. Измеряется в герцах (Hz)
*   **Q** - (добротность) - ширина полосы вокруг выбранной частоты, к которой будет применяться усиление или ослабление. Чем выше значение Q, тем уже полоса. Чем ниже, чем шире.
*   **Gain** - уровень усиления или ослабления данной частоты. Измеряется в децибелах (dB). Увеличение мощности сигнала в 2 раза равно 3dB. В 4 раза - 6dB. В 8 раз - 9dB и т.д.

Тут нужно сказать, что не все параметры актуальны для конкретного типа фильтра (подробнее можно посмотреть <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#BiquadFilterNode" target="_blank" rel="nofollow">тут</a>)

 **Если вы напуганы обилием новых слов - не расстраивайтесь!**

На деле все обстоит намного проще чем в теории. Гоу пробовать живой пример и разбираться изменяя параметры. Гарантирую что все станет намного понятнее.